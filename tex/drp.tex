\documentclass{article}

\usepackage{preamble}

\begin{document}

% \textbf{Metropolis-Hastings Algorithm.} Suppose that we have a Markov chain with transition matrix $P$ with a limiting distribution $\pi$. A sufficient condition for such to hold is (1) \textit{detailed balance} 
% $$P(x) P(x' \mid x) = P(x') P(x \mid x')$$
% and (2) \textit{ergodicity}, i.e., that the chain be aperiodic and positive recurrent. We have that 
% \begin{align*}
%     P(x' \mid x) P(x) &= P(x' \mid x) P(x) \\
%     \frac{P(x' \mid x)}{P(x \mid x')} &= \frac{P(x')}{P(x)}.
% \end{align*}
% Denote $g(x' \mid x)$ the proposal distribution and $A(x', x)$ the acceptance probability. We then have that 
% $$P(x' \mid x) = g(x' \mid x) A(x', x),$$
% so 
% $$\frac{A(x', x)}{A(x, x')} = \frac{P(x')}{P(x)} \frac{g(x' \mid x)}{g(x \mid x')}.$$
% Take the choice 
% $$A(x', x) = \min\left(1, \frac{P(x')}{P(x)} \frac{g(x' \mid x)}{g(x \mid x')}\right).$$
% Observe then that 
% $$0 \leq A(x', x) \leq 1.$$
% The Metroplis-Hastings algorithm is then as follows.
% \begin{enumerate}
% \item[(1)] Intialize.
% \subitem(i) Pick an initial state $x_{0}$.
% \subitem(ii) Set $t = 0$.
% \item[(2)] Iterate.
% \subitem(i) Generate a random candidate state $x'$ according to $g(x' \mid x_{t})$.
% \subitem(ii) Calculate the acceptance probability $A(x', x_{t})$.
% \subitem(iii) Generate a uniform random number $u \in [0, 1]$.
% \subitem(iv) If $u \leq A(x', x_{t})$, then accept the new state and set $x_{t+1} = x'$.
% \subitem(v) Otherwise, reject the new state and set $x_{t+1} = x_{t}$.
% \subitem(vi) Increment $t = t+1$.
% \end{enumerate}
% In particular, we are interested in sampling from the Gibbs distribution 
% $$\pi(x) = \frac{1}{Z_{\beta}} e^{-\beta H(x)}$$
% for $x \in S$ where $Z_{\beta}$ is the partition function 
% $$Z_{\beta} = \sum_{y \in S} e^{-\beta E(y)}$$
% for an energy function $E: S \to \mathbb{R}$ and an inverse temperature $\beta > 0$. Recall that we have the likelihood function 
% $$L(\sigma) = P(\sigma^{-1}(b_{1}))\prod_{j=1}^{n-1}Q(\sigma^{-1}(b_{j+1}) \mid \sigma^{-1}(b_{j})).$$
% Define the energy function 
% \begin{align*}
%     E(\sigma) &= -\log L(\sigma) \\
%     &= -\log P(\sigma^{-1}(b_{1})) - \sum_{j=1}^{n-1} \log Q(\sigma^{-1}(b_{j+1}) \mid \sigma^{-1}(b_{j})).
% \end{align*}

\textbf{Vigenere Cipher.} 

Suppose that our key is a sequence $(k_{1}, \ldots k_{n}, k_{1}, \ldots, k_{n}, \ldots)$ for some key length $n \in Z_{+}$. The keyspace $K$ has cardinality $m^{n}$ where $m$ is the size of the alphabet $A$, say $m = 27$. For a text of length $N$, a cipher is then a function $\sigma: A^{N} \to A^{N}$ with the rule 
$$\sigma_{i}(b_{i}) = (b_{i} + k_{i}) \mod m$$
for $i = 1, \ldots N$. The likelihood function is the same as before, that is, 
$$L(\sigma) = P(\sigma_{1}^{-1}(b_{1}))\prod_{j=1}^{n-1}Q(\sigma_{j+1}^{-1}(b_{j+1}) \mid \sigma_{j}^{-1}(b_{j})).$$
Our energy function is also the same as before, that is, 
$$E(\sigma) = -\log P(\sigma_{1}^{-1}(b_{1})) - \sum_{j=1}^{n-1} \log Q(\sigma_{j+1}^{-1}(b_{j+1}) \mid \sigma_{j}^{-1}(b_{j})).$$
Suppose that the key length is known. If our current key is $(l_{1}, \ldots, l_{n}, l_{1}, \ldots, l_{n})$, then we may propose a new key by choosing an index $i \in \{1, \ldots n\}$ uniformly at random and then choosing a new letter $l_{i}'$ uniformly at random.

\textbf{Running Key Cipher.}

Suppose that our key is a sequence $(k_{1}, \ldots k_{N})$ for some key length $N \in Z_{+}$, where $N$ is the length of the text. The keyspace $K$ has cardinality $m^{N}$ where $m$ is the size of the alphabet $A$, say $m = 27$. For a text of length $N$, a cipher is then the tuple $(k, \sigma)$, where $\sigma: A^{N} \to A^{N}$ is a function with the rule 
$$\sigma_{i}(b_{i}) = (b_{i} + k_{i}) \mod m$$
for $i = 1, \ldots N$. But now suppose that the key is also of natural language. So our prior for the likelihood of a key $k \in K$ is 
$$P(k) = P(k_{1}) \prod_{j=1}^{N-1} Q(k_{j+1} \mid k_{j}).$$
The likelihood function is the same as before, that is,
$$L(\sigma) = P(\sigma_{1}^{-1}(b_{1}))\prod_{j=1}^{N-1}Q(\sigma_{j+1}^{-1}(b_{j+1}) \mid \sigma_{j}^{-1}(b_{j})),$$
so our posterior is 
$$P(k \mid b) = \frac{L(\sigma) P(k)}{P(b)} \propto L(\sigma) P(k).$$
Our energy function is the negative-logarithm of the posterior, that is, 
$$E(k, \sigma) = -\log P(\sigma_{1}^{-1}(b_{1})) - \sum_{j=1}^{N-1} \log Q(\sigma_{j+1}^{-1}(b_{j+1}) \mid \sigma_{j}^{-1}(b_{j})) - \log(k_{1}) - \sum_{j=1}^{N-1}\log Q(k_{j+1} \mid k_{j})$$
If our current key is $k = (k_{1}, \ldots, k_{N})$, then we may propose a new key by choosing an integer $m \in \{1, \ldots, N\}$ at random and then choosing $m$ distinct indices $i_{1}, \ldots, i_{m} \in \{1, \ldots, N\}$ uniformly at random. We then choose $m$ new letters $k_{i_{1}}', \ldots, k_{i_{m}}'$ uniformly at random. The new key is then $(k_{1}, \ldots, k_{i_{1}}', \ldots, k_{i_{m}}', \ldots, k_{N})$.

\end{document}